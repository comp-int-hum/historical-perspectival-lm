data:
  seq_length: 128
  eval_samples: 8192

model:
  type: "GPT2" # or "GPT2"
  name: "GPT2-705M"
  hidden_size: 1536
  intermediate_size: 3072 # train.py uses default = 4 * hidden_size 
  n_layer: 24
  n_head: 16 # Change this if you're using GPT2

  # NOTE: added this friom other GPT2 files
  resid_pdrop: 0.0 # HF Llama doesn't have dropout
  attn_pdrop: 0.0
  embd_pdrop: 0.0

training:
  lr: 2.5e-4
  batch_size: 128
  num_epochs: 4
  gradient_accumulation_steps: 16
  warmup_steps: 300
  fp16: True
  gpus: "0, 1"

