data:
  seq_length: 128
  eval_samples: 8192

model:
  type: "Llama"
  name: "Llama-58M"
  hidden_size: 512
  intermediate_size: 1024
  n_layer: 16
  n_head: 8


training:
  lr: 2.5e-4
  batch_size: 32
  num_epochs: 6
  gradient_accumulation_steps: 1
  warmup_steps: 200
  fp16: True
  weight_decay: 0.1
  alpha: 0.5
  temperature: 2.0

