import os
import os.path
import inspect
import sys

current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
project_root = os.path.abspath(os.path.join(current_dir, '../..'))
print(f"Project root: {project_root}")
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import json
from perspectival_language_models.steamroller_utils import cpu_task_config, gpu_task_config

from steamroller import Environment


vars = Variables("custom_finetuning.py")
vars.AddVariables(
    # Work directory
    ("ORIGINAL_WORK_DIR", "", "work_finetuning"),
    ("WORK_DIR", "", "work_finetuning"),

    # Random Seed
    ("RANDOM_SEED", "", 42),

    # Wandb settings
    ("USE_WANDB", "", True),
    ("WANDB_PROJECT", "", "perspectival_language_models_finetuning"),

    # Training
    ("DORA_LLAMA_CONFIG", "", "config/dora-llama8B.yaml"),
    ("MODEL_PATH", "", "${DATA_ROOT}/models/llama3/Meta-Llama-3-8B-converted"),

    # STEAMROLLER settings
    ("STEAMROLLER_ENGINE", "", "slurm"),
    ("CPU_QUEUE", "", "parallel"),
    ("CPU_ACCOUNT", "", "tlippin1"),    
    ("GPU_QUEUE", "", "a100"),
    ("GPU_ACCOUNT", "", "tlippin1_gpu"),
    ("GPU_COUNT", "", 1),
    ("MEMORY", "", "64GB")
)

env = Environment(
    variables=vars,
    BUILDERS={
        "TrainTokenizer" : Builder(
            action = (
                "python scripts/train_tokenizer.py "
                "--input ${SOURCES} "
                "--output ${TARGETS}"
            )
        ),
        "GetTokenizer" : Builder(
            action = (
                "python scripts/get_tokenizer.py "
                "--input ${TOKENIZER_NAME} "
                "--output ${TARGETS}"
            )
        ),
        "TokenizeSplit" : Builder(
            action = (
                "python scripts/tokenize_split.py "
                "--input ${SOURCES[0]} "
                "--tokenizer ${SOURCES[1]} "
                "--output ${TARGETS}"
            )
        ),
        "DoraFinetune" : Builder(
            action = (
                "python scripts/dora_finetune.py "
                "--model_name ${MODEL_PATH} "
                "--tokenizer_path ${MODEL_PATH} "
                "--train_data ${SOURCES[0]} "
                "--eval_data ${SOURCES[1]} "
                "--config ${CONFIG} "
                "--random_seed ${RANDOM_SEED} "
                "--use_wandb ${USE_WANDB} "
                "--wandb_project ${WANDB_PROJECT} "
                "--wandb_name ${WANDB_NAME} "
                "--output_dir ${TARGET}"
            )
        ),   
    }
)



Import("training_data")

finetuned_results = {}

for slice in training_data.keys():
    env["WORK_DIR"] = os.path.join(env["ORIGINAL_WORK_DIR"], slice)

    train = training_data[slice]["train"]
    dev = training_data[slice]["dev"]
    test = training_data[slice]["test"]

    tokenizer = env.GetTokenizer(
        source = [],
        target = Dir(f"{env['WORK_DIR']}/tokenizer"),
        TOKENIZER_NAME = env["MODEL_PATH"],
        **cpu_task_config(env, "TrainTokenizer", "00:30:00", "24GB")
    )

    train_data = env.TokenizeSplit(
        source = [train, tokenizer],
        target = Dir(f"{env['WORK_DIR']}/train.pt"),
        **cpu_task_config(env, "TokenizeSplit", "01:30:00", "24GB")
    )

    dev_data = env.TokenizeSplit(
        source = [dev, tokenizer],
        target = Dir(f"{env['WORK_DIR']}/dev.pt"),
        **cpu_task_config(env, "TokenizeSplit", "01:30:00", "24GB")
    )

    test_data = env.TokenizeSplit(
        source = [test, tokenizer],
        target = Dir(f"{env['WORK_DIR']}/test.pt"),
        **cpu_task_config(env, "TokenizeSplit", "01:30:00", "24GB")
    )

    model = env.DoraFinetune(
        source = [train_data, dev_data],
        target = Dir(f"{env['WORK_DIR']}/dora_llama"),
        CONFIG = env["DORA_LLAMA_CONFIG"],
        MODEL_PATH = env["MODEL_PATH"],
        WANDB_NAME = f"Dora_llama_{slice}",
        **gpu_task_config(env, "DoraFinetune", "20:00:00", "24GB")
    )

    finetuned_results[slice] = {
        "train": train_data,
        "dev": dev_data,
        "test": test_data,
        "model": model,
        "tokenizer": tokenizer,
    }
print("Done finetuning models.")
Export(
        {
            "finetuned_results" : finetuned_results,
        }
    )